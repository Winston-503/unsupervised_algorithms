## Dimensionality Reduction

Dimensionality reduction refers to techniques that *reduce the number of input variables* in a dataset.
More input features often make a predictive modeling task more challenging to model, more generally referred to as **the curse of dimensionality**.

High-dimensionality statistics and dimensionality reduction techniques are often used for *data visualization*. Nevertheless these techniques can be used in applied machine learning to simplify a classification or regression dataset in order to better fit a predictive model.

### Principal Component Analysis

To reduce the dimensionality, Principal Component Analysis (usually *PCA*) uses the projection original data into the *principal components*. 
The principal components are orthogonal vectors that describe the maximum amount of residual variation (found using *SVD - Singular Value Decomposition*).

Thus, by choosing the first `N` principal components (where `N < M, M is the number of features`), we move from the M-dimensional space to the N-dimensional space, where the new features are a linear combination of the existing features. 

To select the number of components, the so-called *elbow method* is used - a graph of the cumulative sum of the explained variance is plotted and then we can select the number of components that explains the desired part of information (usually 80% or 95%).

PCA requires data scaling and centering (scikit-learn do it automatically).

Some variances of this algorithm are:
- **Incremental PCA** - when data doesn't fit in memory
- **Randomized PCA** - stochastic algorithm allows to quickly estimate the first N components
- **Kernel PCA** - *kernel trick* allows to perform complex nonlinear projections

### Manifold Learning

Manifold Learning algorithms are based in some distance measure conservation.
These algorithms are reducing space dimensionality while saving distances between objects.

- **LLE**
  
   LLE (*Locally Linear Embedding*) studies the linear connections between data points in the original space, and then tries to move to a smaller dementional space, while maintaining the linear connections.

- **MDS**
  
  MDS (*Multidimensional Scaling*) reduces dimensionality while trying to preserve the distances between the instances.

- **Isomap**
  
  Creates a graph by connecting each instance to its nearest neighbors, then reduces dimensionality while trying to preserve the *geodesic distances* between the instances.

- **t-SNE**
  
  t-SNE stands for *t-distributed Stochastic Neighbor Embedding*.
  Reduces dimensionality with saving the relative distance between points in space - keep simular instances close and dissimilar instances apart. **Most often used for data visualization**.

- **LDA**
  
  LDA (*Linear Discriminant Analysis*) is a classification algorithm, but during training it lears the most discriminative axes between the classes, and these axes can then can be used to define a hyperplane onto wich to project the data. The benefit of this approach is that the projection will keep classes as far apart as possible, so LDA is a good technique to reduce dimensionality before running another classification algorithm.

### Random Projections

As its name suggests, projects the data to a lower-dimensional space using a random linear projection. This may sound crazy, but it turns out that such a random projection is actually vary likely to preserve distances well, as was demonstrated mathematically by *William B. Johnson and Joram Lindenstrauss* in a famous [lemma](https://en.wikipedia.org/wiki/Johnson%E2%80%93Lindenstrauss_lemma). The quality of the dimensionality reduction depends on the number of instances and the target dimensionality, but surprisingly not on the initial dimensionality. Check out the documentation for the [sklearn.random_projection](https://scikit-learn.org/stable/modules/random_projection.html) package for more details.
