## Dimensionality Reduction

Dimensionality reduction refers to techniques that *reduce the number of input variables* in a dataset.
More input features often make a predictive modeling task more challenging to model, what is known as **the Curse of Dimensionality**.

| ![dimensionality_reduction.JPG](../img/dimensionality_reduction.JPG) |
|:--:|
| <b>Dimensionality Reduction Example - 3D swissroll to 2D. [Public Domain](https://commons.wikimedia.org/wiki/File:Lle_hlle_swissroll.png)</b>|

High-dimensionality statistics and dimensionality reduction techniques are often used for *data visualization*. Nevertheless these techniques can be used in applied machine learning to simplify a classification or regression dataset in order to better fit a predictive model.

Methods are commonly divided into:
- **Feature Selection** - find a subset of the input features
- **Feature Projection** (or *Feature Extraction*) - find optimal projection of the original data into some low-dimensional space 

Next we will talk about second group of methods. Check *feature engineering* for more feature selection tools, e.g. LASSO regression, correlation analysis etc.

### Principal Component Analysis

To reduce the dimensionality, Principal Component Analysis (*PCA*) uses the projection of the original data into the *principal components*. 
The principal components are orthogonal vectors that describe the maximum amount of residual variation (they are found using *SVD - Singular Value Decomposition*).

Thus, by choosing the first `N` principal components (where `N < M, M is the number of features`), we move from the M-dimensional space to the N-dimensional space, where new features are a linear combination of the existing features. 

To select the number of components, the so-called *elbow method* is used - after plotting a graph of the cumulative sum of the explained variance and then we can select the number of components that explains the desired ratio of information (usually 80% or 95%).

PCA requires data scaling and centering (*scikit-learn* does it automatically).

There are a lot of popular modifications of this algorithms, but the most popular are:
- *Incremental PCA* - for *online learning* or when data doesn't fit in memory
- *Randomized PCA* - stochastic algorithm, that allows to quickly estimate the first N components
- *Kernel PCA* - *kernel trick* allows to perform complex nonlinear projections

### Manifold Learning

Manifold Learning algorithms are based in some distance measure conservation.
These algorithms are reducing the dimensionality *while saving distances between objects*.

- **LLE**
  
  LLE (*Locally Linear Embedding*) studies the linear connections between data points in the original space, and then tries to move to a smaller dimensional space, while preserving within local neighborhoods.
  There are a lot of modifications of this algorithm, like *Modified Locally Linear Embedding (MLLE)*, *Hessian-based LLE (HLLE)* and others.

- **Isomap**
  
  Isomap (short for Isometric Mapping) сreates a graph by connecting each instance to it's nearest neighbors, and then reduces dimensionality while trying to preserve the *geodesic distances* between the instances.

- **t-SNE**
  
  t-SNE stands for *t-distributed Stochastic Neighbor Embedding*.
  Reduces dimensionality with saving the relative distance between points in space - so it keeps similar instances close to each other and dissimilar instances apart. Most often used for data visualization.

### Autoencoders

We can use neural networks to do dimensionality reduction. Autoencoder is a network that tries to output values that are as similar as possible to the inputs, when the network structure implies *a bottleneck* - a layer where the number of neurons is much fewer than in the input layer.

| ![autoencoder_structure.JPG](../img/autoencoder_structure.JPG) |
|:--:|
| <b>Autoencoder Structure. [Public Domain](https://en.wikipedia.org/wiki/Autoencoder#/media/File:Autoencoder_schema.png)</b>|

If we use a linear activation function, we will get linear dimensionality reduction rules, like PCA. But if we use nonlinear activation functions, we can get more complex latent representations. Unfortunately, we have to have a lot of data. Fortunately, this data is unlabeled.

As for other algorithms, there are a lot of different variations, like:
- *Denoising Autoencoders*
- *Variational Autoencoders*
- *Concolutional Autoencoders* for images
- *Reccurent Autoencoders* for time series or text

### How to choose dimensionality reduction algorithm?

First of all make sure you scaled the data. Almost all dimensionality reduction algorithms require that.

If you reduce dimensionality for *data visualization*, you should try **t-SNE** first of all.

If you have a lot of data, **autoencoders** can help you to find very complex latent representations. 

If you don't have a lot of data, try **PCA for linear** dimensionality reduction and **manifold learning algorithms** (LLE, Isomap and others) **for non-linear dimensionality reduction**. 

TODO image 

Note, that almost every algorithm has many variations, and there are a lot of others less popular algorithms, like:
- *Non-negative matrix factorization (NMF)*
- *Random Projections*
- *Linear Discriminant Analysis (LDA)*
- *Multidimensional Scaling (MDS)*
- and others

