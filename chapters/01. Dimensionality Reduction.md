## Dimensionality Reduction

Dimensionality reduction refers to techniques that *reduce the number of input variables* in a dataset.
More input features often make a predictive modeling task more challenging to model, what is known as **the Curse of Dimensionality**.

In the example below the task is to reduce the number of input features (unroll swissroll from 3D to 2D) and save the largest ratio of information at the same time. This is the essence of this task and these algorithms.

| ![dimensionality_reduction.jpg](../img/dimensionality_reduction.jpg) |
|:--:|
| <b>Dimensionality Reduction Example. [Public Domain](https://commons.wikimedia.org/wiki/File:Lle_hlle_swissroll.png)</b>|

High-dimensionality statistics and dimensionality reduction techniques are often used for *data visualization*. Nevertheless these techniques can be used in applied machine learning to simplify a classification or regression dataset in order to better fit a predictive model.

Methods are commonly divided into:
- **Feature Selection** - find a subset of the input features
- **Feature Projection** (or *Feature Extraction*) - find optimal projection of the original data into some low-dimensional space 

Next we will talk about the second group of methods. Check *feature engineering* for more *feature selection* tools, e.g. LASSO regression, correlation analysis etc. In fact, the following algorithms can also be used as *feature selection* tools with the difference that these will no longer be the original features, but some of their modifications (for example linear combinations in case of *PCA*.)

### Principal Component Analysis

To reduce the dimensionality, Principal Component Analysis (*PCA*) uses the projection of the original data into the *principal components*. 
The principal components are orthogonal vectors that describe the maximum amount of residual variation (they are found using *Singular Value Decomposition (SVD)*).

Thus, by choosing the first `N` principal components (where `N < M, M is the number of features`), we move from the M-dimensional space to the N-dimensional space, where new features are linear combinations of the existing features. 

To select the number of components, the so-called *elbow method* is used.  Plot a graph of the cumulative sum of the explained variance and then select the number of components that explains the desired ratio of information (usually 80% or 95%).

PCA requires data scaling and centering (`sklearn.decomposition.PCA` class does it automatically).

There are a lot of popular modifications of this algorithms, but the most popular are:
- *Incremental PCA* - for *online learning* or when data doesn't fit in memory
- *Randomized PCA* - stochastic algorithm that allows to quickly estimate the first N components
- *Kernel PCA* - *kernel trick* allows to perform complex nonlinear projections

### Manifold Learning

Manifold Learning algorithms are based in some distance measure conservation.
These algorithms are reducing the dimensionality *while saving distances between objects*.

| ![manifold_learning.jpg](../img/manifold_learning.jpg) |
|:--:|
| <b>Comparison of Manifold Learning methods by Scikit Learn. [Image Source](https://scikit-learn.org/stable/auto_examples/manifold/plot_compare_methods.html)</b>|

- **LLE**
  
  LLE (*Locally Linear Embedding*) studies the linear connections between data points in the original space, and then tries to move to a smaller dimensional space, while preserving within local neighborhoods.
  There are a lot of modifications of this algorithm, like *Modified Locally Linear Embedding (MLLE)*, *Hessian-based LLE (HLLE)* and others.

- **Isomap**
  
  Isomap (short for *Isometric Mapping*) —Åreates a graph by connecting each instance to it's nearest neighbors, and then reduces dimensionality while trying to preserve the *geodesic distances* (distance between two vertices in a graph) between the instances.

- **t-SNE**
  
  t-SNE stands for *t-distributed Stochastic Neighbor Embedding*.
  Reduces dimensionality with saving the relative distance between points in space - so it keeps similar instances close to each other and dissimilar instances apart. Most often used for data visualization.

### Autoencoders

We can use neural networks to do dimensionality reduction too. Autoencoder is a network that tries to output values that are as similar as possible to the inputs, when the network structure implies *a bottleneck* - a layer where the number of neurons is much fewer than in the input layer.

| ![autoencoder_structure.jpg](../img/autoencoder_structure.jpg) |
|:--:|
| <b>Autoencoder Structure. [Public Domain](https://en.wikipedia.org/wiki/Autoencoder#/media/File:Autoencoder_schema.png)</b>|

If we use a linear activation function, we will get linear dimensionality reduction rules, like *PCA*. But if we use nonlinear activation functions, we can get more complex latent representations. 
Unfortunately, we have to have a lot of data. Fortunately, this data is unlabeled, so it's usually easy to collect.

As for other algorithms, there are a lot of different variations, like:
- *Denoising Autoencoders* that can help clean up the images or sound
- *Variational Autoencoders*. The latent space is in this case composed by a mixture of distributions instead of a fixed vector
- *Concolutional Autoencoders* for images
- *Reccurent Autoencoders* for time series or text

### How to choose dimensionality reduction algorithm?

First of all make sure you scaled the data. Almost all dimensionality reduction algorithms require that.

If you reduce dimensionality for *data visualization*, you should try **t-SNE** first of all.

If you have a lot of data, **autoencoders** can help you to find very complex latent representations. 

If you don't have a lot of data, try **PCA for linear** dimensionality reduction and **manifold learning algorithms** (*LLE*, *Isomap* and others) **for non-linear dimensionality reduction**. 

| ![dimensionality_reduction_algorithm_selection.JPG](../img/dimensionality_reduction_algorithm_selection.JPG) |
|:--:|
| <b>Dimensionality Reduction Algorithm Selection. Image by Author</b>|

Note, that almost every algorithm has many variations, and there are a lot of others less popular algorithms, like:
- *Non-negative matrix factorization (NMF)*
- *Random Projections*
- *Linear Discriminant Analysis (LDA)*
- *Multidimensional Scaling (MDS)*
- and others

