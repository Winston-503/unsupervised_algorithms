## Clustering

Clustering is the task of dividing the population of unlabeled data points into a number of groups such that data points in the same groups are more similar to other data points in the same group than those in other groups.

Clustering is used in a wide variety of applications, insculing these:
- For customer segmentation and recommender systems
- For anomaly detection
- For semi-supervised learning
- For search engines
- For image segmentation
- For data analysis
  
  When analyze a new dataset it can be helpful to run a clustering algorithm, and then analyze each cluster separately.

- As a dimensionality reduction technique
  
  Once a dataset has been clustered, it is possible to measure each instance's *affinity* (any measure of how well an instance fits into a cluster) with each cluster. Each instances's feature vector can then be replaced with the vector of its cluster affinities. If there are k clusters, then this vector is k-dimensional. This vector is typically much lower-dimensional than the original feature vector, but it can preserve enough information for further processing.


All clustering algorithms **requires data preprocessing and standartization**.
If the number of clusters is unknown, good initial approximation is *the square root of the number of objects*.
Most clustering algorithms perform worse with a large number of features, so it is sometimes recommended to use methods of *dimensionality reduction* before clustering.

Metrics of quality:
- **ARI** (Adjusted Rand Index) if the real classes are known
- **Silhouette** if the real classes are unknown. Acts similar to k-Means, so it doesn't always evaluate correctly.

### K-Means
  
After cluster centroids initializations (random or other) at each step we do following:
- Assign the cluster number with the nearest center to the observations,
- Move the cluster centroids to the average value of the cluster member coordinates.

To choose good number of clusters we can use *sum of squared distances from points to cluster centroids* as metric and choose the number when this metric stopped falling fast. Other metrics that can be used - *inertia* or *silhouette*.

Speeded version of this algorithm is **Mini Batch K-Means**, when we use random subsample of dataset instead of whole dataset for calculations.
There are a lot of others modifications, many of which are implemented in scikit-learn.

**Pros**:
+ Simple and intuitive
+ Scales to large data sets
+ As a result of the algorithm, we have a centroid that can be used as a standart representative of the cluster
  
**Cons**:
- Does not cope well with a very large number of features
- Separates only convex and homogeneous clusters well
- Can end up covering to poor solutions, so it needs to be run several times, keeping only the best solution (`n_init` parameter)

### Hierarchical Clustering

Hierarchical clustering (sometimes **HCA - Hierarchical Cluster Analysis** or **Agglomerative Clustering**) is a general family of clustering algorithms that build nested clusters by merging or splitting them successively. 
This hierarchy of clusters is represented as a tree (or dendrogram). 
The root of the tree is the unique cluster that gathers all the samples, the leaves being the clusters with only one sample. 

According to the generated dendrogram, you can choose separation into any number of clusters.
This family of algorithms requires calculating the distance between clusters. For this purpose, different metrics are used, one of the most popular is **Ward distance**.

**Pros**:
+ Simple and intuitive
+ Works well when data has a hierarchical structure
  
**Cons**:
- Greedy algorithm
- Separates only convex and homogeneous clusters well

### DBSCAN

DBSCAN stands for *Density-Based Spatial Clustering of Applications with Noise*.

The DBSCAN algorithm views clusters as areas of high density separated by areas of low density.
The central component to the DBSCAN is the concept of *core samples*, which are samples that are in areas of high density. A cluster is therefore a set of core samples, each close to each other and a set of non-core samples that are close to a core sample. Others samples are defined as outliers (or anomalies).

An extension or generalization of the DBSCAN algorithm is the **OPTICS** algorithm (Ordering Points To Identify the Clustering Structure).

**Pros**:
+ Selects the number of clusters itself
+ Solves the anomaly detection task at the same time
  
**Cons**:
- Need to select the density parameter 
- Does not cope well with a sparce data

### Affinity Propagation

Affinity Propagation creates clusters by sending messages between pairs of samples until convergence. A dataset is then described using a small number of examples (standart representatives), which are identified as those most representative of other samples. Unfortunately, this algorithm has a computational complexity of O(m^2), so it too is not suited for large datasets.

**Pros**:
+ Selects the number of clusters itself
+ As a result of algorithm we have standart representatives of a cluster. Unlike K-Means these representatives are not just mean value, but real object from train set.
  
**Cons**:
- Computational complexity of O(m^2), so it too is not suited for large datasets
- Separates only convex and homogeneous clusters well
- Usually works worse than other algorithms

### Mean Shift

This algorithm starts by placing a circle centered on each instance, then for each circle it computes the mean of all the instances located within it, and it shifts the circle so that it is centered on the mean. Next, it iterates this mean-shifting step until all the circles stop moving (i.e., until each of them is centered on the mean on the instances it contains).

Mean-Shift shifts the circles in the direction of higher density, until each of them has found a local density minimum. Finally, all the instances whose circles have settled in the same place (or close enough) are assigned to the same cluster. Mean-Shift has some of the features of DBSCAN, because it's base on density too.

**Pros**:
+ Selects the number of clusters itself
+ Have just one hyperparameter - the radius of the circles, called *bandwidth*
  
**Cons**:
- Does not cope well with a sparce data
- Tends to chop clusters into pieces when they have internal density variations
- Computational complexity of O(m^2), so it too is not suited for large datasets

### Spectral clustering

This algorithm takes a similarity matrix between the instances and creates a low-dimensional embedding from it (i.e., reduces its dimensionality), then is uses another clustering algorithm in this low-dimensional space (Scikit-Learn implementation uses K-Means).

Spectral clustering can capture complex cluster structures and it can also be used to cut graphs (e.g. to identify clusters of friends on a social network). It does not scale well large numbers of instances, and it does not 

**Pros**:
+ Can capture complex cluster structures
+ Can also be used to cut graphs (e.g., to identify clusters of friends on a social network)
  
**Cons**:
- Does not scale well large numbers of instances
- Does not behave well when the clusters have very different sizes

### BIRCH

The BIRCH stands for *Balanced Iterative Reducing and Clustering using Hierarchies*. This algorithms was designed specifially for very large datasets, and it can be faster than batch K-Means, with simular results, as long as the number of features is not too large (<20). During training, it builds a tree structure containing just enough information to quickly assign each new instance to a cluter, without having to store all the instances in the tree: this approach allows it to use limited momory, while handling huge datasets.

**Pros**:
+ Was designed specifially for very large datasets when number of features is not too large (<20)
+ Allows it to use limited momory, while handling huge datasets

### Gaussian Mixture Models

Gaussian Mixture Models (*GMM*) is a probabilistic model that can solve these unsupervised learning tasks:
- *Clustering*
- *Anomaly detection*
- *Density Estimation*

This alorithm assumes that the data instances were generated from a mixture of several Gaussian distributions whose parameters are unknown.
This method relies on the **EM - Expectation Maximization** algorithm.

To choose good number of clusters we can use **BIC** (Bayesian information criterion) or **AIC** (Akaike information criterion) and choose the model with minimum value. On the other hand, **Bayesian GMM** can be used - this model can detect the number of clusters itself and requires only value that is greater than the optimal number of clusters.

Gaussian mixture model is a *generative model*, meaning you can sample new instances from it. It is also possible to estimate the density of the model at any given location.

Pros:
+ Perfectly deals with data instances that were generated from a mixture of Gaussian distributions with different shapes and sizes
  
Cons:
- Requires the number of clusters (not in case Bayesian GMM)
- *EM* can end up covering to poor solutions, so it needs to be run several times, keeping only the best solution (`n_init` parameter)
- Does not scale well large numbers of features
- Assume that data instances were generated from a mixture of Gaussian distributions
