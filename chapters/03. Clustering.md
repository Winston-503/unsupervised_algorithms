## Clustering

Clustering is the task of dividing the population of unlabeled data points into a number of groups in such a way that objects in the same group (called *a cluster*) are more similar to each other than to those in other clusters.

| ![clustering.jpg](../img/clustering.jpg) |
|:--:|
| <b>Clustering Algorithms by Scikit Learn. [Image Source](https://scikit-learn.org/0.15/auto_examples/cluster/plot_cluster_comparison.html)</b>|

Clustering is used in a wide variety of applications, including these:
- Semi-supervised learning
- Data analysis - when analyzing a new dataset it can be helpful to run a clustering algorithm, and then analyze each cluster separately
- For anomaly detection - instance, that doesn't belong to any cluster can be considered an anomaly
- Customer segmentation, recommender systems, search engines, image segmentation etc.

All clustering algorithms **requires data preprocessing and standartization**.
Most clustering algorithms perform worse with a large number of features, so it is sometimes recommended to use methods of *dimensionality reduction* before clustering.

### K-Means

Algorithm starts with random (or not random) *centroids* initializations, which are used as the beginning points for every cluster. After that we iteratively do the following:
- Assign the observations a cluster number with the nearest center, and
- Move the cluster centroids to the new average value of the cluster elements.

| ![k-means.jpg](../img/k-means.jpg) |
|:--:|
| <b>K-Means Clustering by Scikit Learn. [Image Source](https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_digits.html)</b>|

To choose a good number of clusters we can use *sum of squared distances from points to cluster centroids* as metric and choose the number when this metric stopped falling fast. Other metrics that can be used - *inertia* or *silhouette*.

Speeded version of this algorithm is **Mini Batch K-Means**, when we use a random subsample of the dataset instead of the whole dataset for calculations. There are a lot of other modifications, many of which are implemented in *scikit-learn*.

**Pros**:
+ Simple and intuitive
+ Scales to large datasets
+ As a result of the algorithm, we have a centroid that can be used as a standard representative of the cluster
 
**Cons**:
- The number of clusters must be specified
- Does not cope well with a very large number of features
- Separates only convex and homogeneous clusters well
- Can end up covering to poor solutions, so it needs to be run several times, keeping only the best solution (`n_init` parameter in *sklearn*)

### Hierarchical Clustering

Hierarchical clustering (sometimes **Hierarchical Cluster Analysis (HCA)** or **Agglomerative Clustering**) is a general family of clustering algorithms that build nested clusters by merging or splitting them successively. 
This hierarchy of clusters is represented as a tree (or dendrogram). 
The root of the tree is the unique cluster that gathers all the samples, and the leaves are the clusters with only one sample.

| ![dendrogram.jpg](../img/dendrogram.jpg) |
|:--:|
| <b>Hierarchical Clustering Dendrogram Example by Scikit Learn. [Image Source](https://scikit-learn.org/stable/auto_examples/cluster/plot_agglomerative_dendrogram.html)</b>|

According to the generated dendrogram, you can choose desired separation into any number of clusters.
This family of algorithms requires calculating the distance between clusters. For this purpose, different metrics are used, one of the most popular is **Ward distance**.

**Pros**:
+ Simple and intuitive
+ Works well when data has a hierarchical structure
  
**Cons**:
- The number of clusters must be specified
- Greedy algorithm
- Separates only convex and homogeneous clusters well

### DBSCAN

DBSCAN stands for *Density-Based Spatial Clustering of Applications with Noise*.

The DBSCAN algorithm views clusters as areas of high density separated by areas of low density.
The central component to the DBSCAN is the concept of *core samples*, which are samples that are in areas of high density. A cluster is therefore a set of core samples, each close to each other and a set of non-core samples that are close to a core sample. Other samples are defined as outliers (or anomalies).

An extension or generalization of the DBSCAN algorithm is the **OPTICS** algorithm (Ordering Points To Identify the Clustering Structure).

**Pros**:
+ You don't need to specify the number of clusters
+ Solves the anomaly detection task at the same time
 
**Cons**:
- Need to select the density parameter
- Does not cope well with a sparse data

### Affinity Propagation

Affinity Propagation creates clusters by sending messages between pairs of samples until convergence. A dataset is then described using a small number of examples (standart representatives), which are identified as those most representative of other samples.

| ![affinity_propagation.jpg](../img/affinity_propagation.jpg) |
|:--:|
| <b>Affinity Propagation Clustering by Scikit Learn. [Image Source](https://scikit-learn.org/stable/auto_examples/cluster/plot_affinity_propagation.html)</b>|

Unfortunately, this algorithm has a computational complexity of O(m^2), so it too is not suited for large datasets.

**Pros**:
+ You don't need to specify the number of clusters
+ As a result of the algorithm we have standart representatives of a cluster. Unlike K-Means these representatives are not just mean values, but real objects from the train set.
 
**Cons**:
- Computational complexity of O(m^2), so it too is not suited for large datasets
- Separates only convex and homogeneous clusters well
- Usually works worse than other algorithms

### Mean Shift

This algorithm starts by placing a circle centered on each instance, then for each circle it computes the mean of all the instances located within it, and it shifts the circle so that it is centered on the mean. Next, it iterates this mean-shifting step until all the circles stop moving.

Mean Shift shifts the circles in the direction of higher density, until each of them has found a local density minimum. Finally, all the instances whose circles have settled in the same place (or close enough) are assigned to the same cluster. Mean Shift has some of the features of DBSCAN, because it's based on density too.

**Pros**:
+ You don't need to specify the number of clusters
+ Have just one hyperparameter - the radius of the circles, called *bandwidth*
 
**Cons**:
- Does not cope well with a sparse data
- Tends to chop clusters into pieces when they have internal density variations
- Computational complexity of O(m^2), so it too is not suited for large datasets

### BIRCH

The BIRCH stands for *Balanced Iterative Reducing and Clustering using Hierarchies*. This algorithm was designed specifically for very large datasets, and it can be faster than batch K-Means, with similar results, as long as the number of features is not too large (<20). During training, it builds a tree structure containing just enough information to quickly assign each new instance to a cluster, without having to store all the instances in the tree: this approach allows it to use limited memory, while handling huge datasets.

**Pros**:
+ Was designed specifically for very large datasets when number of features is not too large (<20)
+ Allows it to use limited memory, while handling huge datasets

**Cons**:
- The number of clusters must be specified
- Does not cope well with a high-dimensional data

### Gaussian Mixture Models

Gaussian Mixture Models (*GMM*) is a probabilistic model that can solve both *Clustering* and *Anomaly detection*/*Density Estimation* unsupervised learning tasks.

This method relies on the *Expectation Maximization (EM)* algorithm and assumes that the data instances were generated from a mixture of several Gaussian distributions whose parameters are unknown.

| ![GMM.jpg](../img/GMM.jpg) |
|:--:|
| <b>Density Estimation with GMM by Scikit Learn. [Image Source](https://scikit-learn.org/stable/auto_examples/mixture/plot_gmm_pdf.html)</b>|

To choose a good number of clusters we can use *BIC* (Bayesian information criterion) or *AIC* (Akaike information criterion) and choose the model with minimum value. On the other hand, **Bayesian GMM** can be used - this model can detect the number of clusters itself and requires only a value that is greater than the optimal number of clusters.

The Gaussian Mixture Model is a *generative model*, meaning you can sample new instances from it. It is also possible to estimate the density of the model at any given location.

Pros:
+ Perfectly deals with data instances that were generated from a mixture of Gaussian distributions with different shapes and sizes
+ At the same time solves *density estimation* task
+ Is a *generative model*
 
Cons:
- The number of clusters must be specified (not in case of *Bayesian GMM*)
- *Expectation Maximization* algorithm can end up covering to poor solutions, so it needs to be run several times, keeping only the best solution (`n_init` parameter in *sklearn*)
- Does not scale well large numbers of features
- Assume that data instances were generated from a mixture of Gaussian distributions, so cope bad with data of other shape

### How to choose clustering algorithm?

As you can see, clustering task is quite difficult and have a wide variety of applications, so it's almost impossible to build some universal set of rules to select a clustering algorithm - all of them have advantages and disadvantages.

Things become better when you have some assumptions about your data, so *data analysis* can help you with that. What is the approximate number of clusters? Are they located far from each other or do they intersect? Are they similar in shape and density? All that information can help you to solve your task better.

If the number of clusters is unknown, a good initial approximation is *the square root of the number of objects*. You can also first run an algorithm that does not require a number of clusters (*DBSCAN* or *Affinity Propagation*) and start from this number.

But the most important question remains the evaluation of quality - you can try all the algorithms, but how to understand which one is the best? There are a great many metrics for this - from *homogeneity* and *completeness* to *silhouette* - they show themselves differently in different tasks.
